---
title: "Untitled"
format: html
editor: visual
---

Overfitting?

```{r, eval=F}
# did i delete residuals_df? 

resid_by_elec <- residuals_df %>%
  group_by(id_elec, model) %>%
  filter(model %in% best_models) %>% 
  summarise(mean_resid = mean(resid, na.rm=TRUE),
            sd_resid = sd(resid, na.rm=TRUE),
            .groups="drop")

ggplot(resid_by_elec, aes(x = id_elec, y = mean_resid, colour = model, group = model)) +
  geom_line() + geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(title = "Average residuals by election",
       y = "Mean Residual (Bias)", x = "Election") +
  theme_minimal()

```

```{r, eval=F}
ggplot(residuals_df, aes(x = resid, fill = model)) +
  geom_density(alpha = 0.3) +
  facet_wrap(~ model, scales = "free_y") +
  labs(title = "Residual distributions by model", x = "Residual", y = "Density") +
  theme_minimal()

# fat tails or asymmetry may suggest poor fit

```

### Prediction intervals

To assess the reliability of our predictions, we construct prediction intervals to quantify the uncertainty around each predicted value.

-   Compare how tight or wide the intervals are
-   See how often true values fall outside the interval (coverage error)
-   Visualise model reliability & robustness

Considerations:

-   Consistency: always have to use the same residual logic (same quantile from same calibration set) across models

-   Robustness: ensure clean calibration subset (residuals\[1:100\] works good =\> using the first 100 residuals as a reference for estimating uncertainty)

    -   To construct prediction intervals, we followed the conformal prediction approach provided in class, using the first 100 residuals as a reference for estimating uncertainty. However, the original implementation applied the 5th and 95th quantiles of raw residuals directly, which led to overly wide or inaccurate intervals. To improve reliability, we adjusted the method by applying symmetric quantiles of absolute residuals instead. This preserves the spirit of conformal prediction while ensuring the intervals better reflect typical prediction error. The adjustment avoids overfitting to early residual noise and yields more interpretable, balanced intervals.

-   Validity: works even if models are of different types (e.g. lm, lmer, RF, NN, KNN) or if the models do not natively support interval prediction (ML models usually do not)

-   Interpretability: approach does not give true confidence intervals, but empirical error bounds =\> fine for practical evaluation

```{r, eval=F}
# for individual model_x
y <- pred_store$obs
yhat = pred_store$model_X

lwr <- yhat[101:length(yhat)] - quantile(abs(noise), 0.95, na.rm = T)
upr <- yhat[101:length(yhat)] + quantile(abs(noise), 0.95, na.rm = T)

predictions = data.frame(real = y[101:length(y)],
                         fit = yhat[101:length(yhat)], 
                         lwr = lwr, upr = upr) %>%
  mutate(out = factor(if_else(real < lwr | real > upr, 1, 0)))

cat("Model_X:", mean(predictions$out == 1), "\n")

```

```{r}
# loop function
calculate_PI <- function(pred_store, model_name) {
  
  d <- pred_store %>% 
    filter(model == model_name) %>% 
    arrange(row_id)
  
  # base vectors 
  y <- d$obs
  yhat <- d$pred
  
  # residuals and calibration noise from first 100
  error <- y - yhat
  noise <- error[1:100]
  q <- quantile(abs(noise), 0.95, na.rm = TRUE)
  
  # values
  real_vals <- y[101:length(y)] 
  pred_vals <- yhat[101:length(y)]
  
  # prediction interval (symmetric around prediction)
  lwr <- pred_vals - q
  upr <- pred_vals + q
  
  # prediction dataframe 
  predictions <- tibble(
    model = model_name,
    real = real_vals,
    fit = pred_vals,
    lwr = lwr,
    upr = upr
  ) %>% 
    mutate(out = real < lwr | real > upr, 
           coverage_error = mean(out, na.rm = TRUE)
           ) 
  
}
```

```{r}
# loop for all models 
model_names <- unique(pred_store$model)

# storing prediction intervals and coverage
pi_list <- map(model_names, ~ calculate_PI(pred_store, .x))

# prediction interval data (=> used for plots)
all_intervals <- bind_rows(pi_list)

# resuts coverage
coverage_summary <- all_intervals %>%
  group_by(model) %>%
  summarise(coverage_error = mean(out, na.rm = TRUE), .groups = "drop")

coverage_summary %>% 
  arrange(coverage_error)
```

visualisations:

```{r}
# all 
ggplot(all_intervals, aes(x = fit, y = real)) +
  geom_point(aes(colour = out), alpha = 0.6) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.2) +
  facet_wrap(~ model) + # try with  scales = "free"
  labs(title = "Prediction Intervals by Model",
       x = "Predicted", 
       y = "Observed") +
  theme_minimal()

# individual 
ggplot(filter(all_intervals, model == "m_lm_before"), aes(x = fit, y = real)) +
  geom_point(aes(colour = out)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3) +
  labs(title = "Prediction Intervals for LM Before",
       x = "Predicted", y = "Observed") +
  theme_minimal() + 
  theme(legend.position="none")

```

The plot shows the prediction intervals. Observed values (in red) fall within the shaded confidence band, confirming good coverage. Outliers (in blue) lie outside the interval and suggest cases where the model under- or overestimates. These could point to unmodelled factors or areas where the model is less reliable.

### DummyPollster

As we will use polling-firm as a categorical variable in some of the descriptive and predictive models, we need a DummyPollster, a baseline reference level that represents a neutral pollster (an artifical baseline) whose estimate reflects the average pollster bias across the whole dataset for each party. Hence it is calculated as the average of all pollster predictions for each party. It serves as a baseline to compare the different pollsters coefficients (errors).

```{r}
error_avg_global <- data_2023_desc %>%
  group_by(abbrev_candidacies) %>% 
  summarise(
    error_avg_dummy = mean(estimated_voting - voting_results_pct)
    )  

error_avg_global
```

The global average error is:

-   EH-BILDU: -0.154, ERC: 1.15, JXCAT-JUNTS: 0.511, PNV: 0.374, PP: -5.46, PSOE: -4.98, SUMAR: 0.504, VOX: 3.27

We can now create a dummy pollster inside polling_firm with an estimated_voting that is the actual result + global average error for each party

```{r}
dummy_rows <- data_2023_desc %>%
  left_join(error_avg_global, by = "abbrev_candidacies") %>%
  mutate(
    polling_firm = "DummyPollster",
    estimated_voting = voting_results_pct + error_avg_dummy
  ) %>% 
  select(-error_avg_dummy)

dummy_rows

data_2023_desc <- bind_rows(data_2023_desc, dummy_rows)
```

Releveling:

```{r}
data_2023_desc <- data_2023_desc %>%
  mutate(polling_firm = relevel(factor(polling_firm), ref = "DummyPollster")) 

```

### Mixed models

TODO:

Using the benchmark correction instead moves us away from pollster identity and toward systematic bias patterns that can generalise.

TODO: test with mixed_effects

If we wanted to use the simple average firm-party house effects (truth-anchored), we would need to hard-code polling_firm and abbrev_candidacies into the models, so the model would learn that “Firm X always overestimates Party Y by +2”. That is possible with mixed-effects models, but: for unseen pollsters/parties in the future, this would not generalise.

CV

```{r, eval=F}
set.seed(70901) 

test_results <- data.frame(actual = df_2023$voting_results_pct)
```

Mixed-effects - correction during modeling: bias as random variation

without abbrev_candidacies

```{r, eval=F}
# training
mm_during <- lmer(voting_results_pct ~ 
                    estimated_voting + correction_final + 
                    n_field_days + 
                    sample_size + 
                    party_elec_count + 
                    first_time +
                    (1 | polling_firm),
                  data = df_hist)

# testing
test_results$mm_during <- predict(mm_during, newdata = df_2023, allow.new.levels = TRUE)

saveRDS(mm_during, file = "mm_during_model.rds")
```

Mixed-effects - correction before modeling: test for residual variation // residual correction?

without abbrev_candidacies

```{r, eval=F}
# training
mm_before <- lmer(voting_results_pct ~ 
                    debiased_estimate_final + 
                    n_field_days + 
                    sample_size + 
                    party_elec_count + 
                    first_time +
                    (1 | polling_firm),
                  data = df_hist)

# testing
test_results$mm_before <- predict(mm_before, newdata = df_2023, allow.new.levels = TRUE)

saveRDS(mm_before, file = "mm_before_model.rds")

```

abbrev_candidacies:

DOESNT WORK:

```{r, eval=F}
mm_during_w <- lmer(voting_results_pct ~ 
                      estimated_voting + correction_final + 
                      n_field_days + 
                      sample_size + 
                      party_elec_count + 
                      first_time +
                      abbrev_candidacies + 
                    (1 | polling_firm),
                    data = df_hist)

# testing
test_results$mm_during_w <- predict(mm_during_w, newdata = df_2023, allow.new.levels = TRUE)

saveRDS(mm_during_w, file = "mm_during_model_w.rds")

# training
mm_before_w <- lmer(voting_results_pct ~ 
                      debiased_estimate_final + 
                      n_field_days + 
                      sample_size + 
                      party_elec_count + 
                      first_time +  
                      abbrev_candidacies +
                    (1 | polling_firm),
                   data = df_hist)

# testing
test_results$mm_before_w <- predict(mm_before_w, newdata = df_2023, allow.new.levels = TRUE)

saveRDS(mm_before, file = "mm_before_model_w.rds")
```

“non-conformable arguments” at predict() with lmer because the model matrix built from df_2023 does not match what the model was trained on =\> that happens when:

-   a factor in newdata has levels not seen in training (very likely for abbrev_candidacies, e.g., SUMAR in 2023), or
-   the factor levels/order don’t match

We check for new/absent levels:

```{r, eval=F}
# factors used in fixed/random parts
facs <- c("polling_firm", "abbrev_candidacies")

for (v in facs) {
  cat("\n--", v, "--\n")
  tr_lvls <- levels(df_hist[[v]])
  te_lvls <- levels(factor(df_2023[[v]]))
  cat("new in TEST only:", setdiff(te_lvls, tr_lvls), "\n")
  cat("missing in TEST:",  setdiff(tr_lvls, te_lvls), "\n")
}

```

"new in TEST only” (e.g., a party that never appeared historically) =\> predict() with abbrev_candidacies in the fixed effects will fail =\> allow.new.levels=TRUE only helps for random effects, not fixed factors

WORKS:

```{r, eval=F}
mm_during_w <- lmer(voting_results_pct ~ 
                      estimated_voting + correction_final + 
                      n_field_days + 
                      sample_size + 
                      party_elec_count + 
                      first_time +
                    (1 | polling_firm) + (1 | abbrev_candidacies),
                    data = df_hist)

# testing
test_results$mm_during_w <- predict(mm_during_w, newdata = df_2023, allow.new.levels = TRUE)

saveRDS(mm_during_w, file = "mm_during_model_w.rds")

# training
mm_before_w <- lmer(voting_results_pct ~ 
                     debiased_estimate_final + 
                     n_field_days + 
                     sample_size + 
                     party_elec_count + 
                     first_time + 
                    (1 | polling_firm) + (1 | abbrev_candidacies),
                   data = df_hist)

# testing
test_results$mm_before_w <- predict(mm_before_w, newdata = df_2023, allow.new.levels = TRUE)

saveRDS(mm_before, file = "mm_before_model_w.rds")
```

testing:

```{r, eval=F}

results_reg <- test_results %>% 
  select(-actual) %>% 
  map_df(~ postResample(pred = ., obs = test_results$actual), .id = "model")

results_reg


library(knitr)
kable(results_reg, digits = 3, caption = "Out-of-sample predictive performance of regression models")



test_results %>%
  pivot_longer(-actual, names_to = "model", values_to = "predicted") %>% 
  ggplot(aes(x = predicted, y = actual)) +
  geom_point(alpha = 0.4) +
  geom_abline(slope = 1, intercept = 0, colour = "darkblue", linewidth = 0.8) +
  facet_wrap(~model, scales = "free") +
  labs(
    title = "Observed vs Predicted Vote Share",
    x = "Predicted",
    y = "Actual Vote Share"
  ) +
  theme_minimal()
```

mm_during_w =\> 0.97%

coverage error:

```{r, eval=F}
# loop for all models 
model_names <- names(test_results)#[names(test_results) != "actual"] ! we want to keep the baseline model 


# loop function
calculate_PI <- function(model_name, test_results) {
  
  # base vectors 
  y <- test_results$actual
  yhat <- test_results[[model_name]]
  
  # residuals and calibration noise from first 100
  error <- y - yhat
  noise <- error[1:100]
  q <- quantile(abs(noise), 0.95, na.rm = TRUE)
  
  # values
  real_vals <- y[101:length(yhat)] # yhat instead of y as some models might have fewer predictions than y (e.g. due to missing values or filtering) => we guarantee alignment between predicted (yhat) and actual (y) values # TODO: add a defensive check (e.g. stopifnot(length(yhat) == length(y))) to catch those mismatches early??
  pred_vals <- yhat[101:length(yhat)]
  
  # prediction interval (symmetric around prediction)
  lwr <- pred_vals - q
  upr <- pred_vals + q
  
  # prediction dataframe 
  predictions <- data.frame(
    model = model_name,
    real = real_vals,
    fit = pred_vals,
    lwr = lwr,
    upr = upr
  ) %>% 
    mutate(out = factor(if_else(real < lwr | real > upr, 1, 0))) # predictions outside the interval
  
  # coverage rate
  coverage_error <- mean(predictions$out == 1, na.rm = TRUE)
  
  # output 
  return(list(predictions = predictions, coverage = coverage_error))
}

# storing prediction intervals and coverage
pi_results <- map(model_names, ~ calculate_PI(.x, test_results))

# coverage rates 
coverage_summary <- tibble(
  model = model_names,
  coverage_error = map_dbl(pi_results, "coverage")
)

# prediction interval data (=> used for plots)
all_intervals <- map_dfr(pi_results, "predictions")

# resuts coverage
coverage_summary %>% 
  arrange(coverage_error)

ggplot(all_intervals, aes(x = fit, y = real)) +
  geom_point(aes(colour = out), alpha = 0.6) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.2) +
  facet_wrap(~ model) +
  labs(title = "Prediction Intervals by Model",
       x = "Predicted", 
       y = "Observed") +
  theme_minimal()

```

# Descriptive modeling

Model 1: Party characteristics (e.g., party_elec_count, first_time) explain some variation in polling estimates, but not much (R^2^ \~ 0.14).

Model 2: Including correction_final substantially improves fit (R^2^ \~ 0.41) =\> house effects are systematic and predictable.

Model 4: After correction, pollster effects disappear =\> the benchmark correction works as intended. Residual bias is minimal.

1.  Baseline model without correction or house effect

-   Aim: show how inaccurate raw estimates can be, providing motivation for introducing corrections.
-   Formula: `lm(estimated_voting ~ n_field_days + ...)` (without correction_final, abbrev_candidacies or polling_firm)

2.  Model including correction_final as predictor

-   Aim: show that pollster bias is not random but systematic and predictable (pollsters make recurring mistakes)
-   Formula: `lm(estimated_voting ~ correction_final + n_field_days + ...)`

4.  Interaction model with debiased estimates: check for residual bias after the correction (whether debiasing worked) (used as a diagnostic)

-   Aim: test whether the correction method neutralised bias or whether residual polling behaviour remains (= if the estimates themselves are still biased post-correction). If polling_firm is not significant, suggests neutralisation of pollster differences, vs if polling_firm is still significant, suggests residual bias and that the correction does not capture everything.

    -   Question: after correction, do pollsters still differ systematically in their estimated support for each party?
    -   In theory: if the correction is fully effective, then the pollster should no longer matter and including polling_firm should add no explanatory power.
    -   In practice: if the debiasing is imperfect, this model can test whether residual house effects remain after correction.

-   Formula: `lm(debiased_estimate ~ polling_firm * abbrev_candidacies)`

### 1. Baseline model without correction or house effect

```{r}
desc_m1_baseline <- lm(estimated_voting ~ 
                         n_field_days + 
                         sample_size + 
                         party_elec_count + 
                         first_time,
                       data = data_2023_desc)

summary(desc_m1_baseline)

```

Aim: show that uncorrected polling estimates contain bias and that correction is necessary.

> This model provides a baseline for how much of the variation in raw, uncorrected polling estimates can be explained without correcting for house effects (correction_final, polling_firm) or accounting for party identity (abbrev_candidacies). It serves to motivate why more sophisticated correction is necessary and sets a benchmark for assessing improvement from house-effect corrections.

Key findings:

-   Very strong effects for party-level predictors:

    -   `sample_size`: Negative and significant (β = –0.354, p \< .01). Larger samples are associated with slightly lower estimated vote shares. Could suggest that smaller polls overestimate or are noisier.
    -   `party_elec_count`: Strongly positive and significant (β = +4.35, p \< .001). Parties that have contested more elections tend to get higher estimates.
    -   `first_time`: Positive and significant (β = +1.41, p \< .001). First-time parties are estimated to receive higher support, possibly reflecting novelty effects or overestimation due to lack of past baseline.
    -   `n_field_days`: Not significant (β = –0.039, p = .742).

-   Model fit:

    -   `R^2^ = 0.137`: The model explains \~14% of the variance in uncorrected polling estimates, a weak effect, , suggesting that basic structural predictors alone are not sufficient to explain poll results.
    -   This underscores the need to include pollster and party effects in subsequent models, to improve predictive accuracy by accounting for systematic polling biases.
    -   **It supports the need for correction**: the residuals are large (resid SE = 10.47) and unmodeled variance remains high.

Summary: The model provides minimal structure and has very little predictive power, as seen by the low R^2^ (= 0.137).The strong coefficients for party_elec_count and first_time suggest that party characteristics do matter, but not enough to explain poll inaccuracies. This model sets a benchmark: later models should outperform it significantly, especially when accounting for systematic polling distortions (house effects).

### 2. Model including correction_final as predictor

```{r}
desc_m2_with_bias <- lm(estimated_voting ~ 
                          correction_final +
                          n_field_days + 
                          sample_size + 
                          party_elec_count + 
                          first_time, 
                        data = data_2023_desc)

summary(desc_m2_with_bias)
```

Aim: show that pollster bias is not random but systematic and predictable (pollsters make recurring mistakes)

> This model tests the assumption that pollster bias is systematic, not random. If house effects (captured here as `correction_final`) are significant and improves explanatory power, it suggests that pollsters make predictable, recurring mistakes which suggests that their correction is both needed and feasible, thus justifying their inclusion.

Key findings:

-   **correction_final has a strong significant effect**

    -   Coefficient: −2.06, p \< .001
    -   For each additional percentage point of average overestimation (i.e., higher bias), the poll estimate is about 2 points higher =\> indicating systematic inflation by biased pollsters = **the more a pollster overestimates on average, the lower the actual estimated support ends up being, reflecting a consistent over-correction pattern**. Clear evidence that bias is not random, but patterned and predictable

-   Covariates retain similar effects as in Model 1:

    -   `sample_size`: Negative and significant (β = –0.32, p \< .01). As in Model 1, larger samples yield slightly lower estimated vote shares , may reflect increased precision or conservative estimates in large polls.
    -   `party_elec_count`: Positive and significant (β = +0.54, p \< .001). More experienced parties receive slightly higher estimates, but effect size is weaker than in Model 1 (possibly now partially absorbed by bias effects).
    -   `first_time`: Positive and significant (β = +0.79, p \< .001). Still overestimated even when accounting for bias =\> suggests novelty effects or consistent polling inflation for new entrants.
    -   `n_field_days`: Not significant (β = –0.17, p = .08)

-   Model fit improves drastically:

    -   R^2^ increases from 0.137 to 0.410 = the model explains 41% of the variance in estimated vote shares.
    -   That is a +27 percentage point gain in explained variance by adding just one variable (correction_final), underscoring the central importance of house effects. It confirms that polling bias is systematic and measurable.
    -   Including correction_final reduces residual variance (Res se: from 10.47 to 8.66) and provides clear justification for integrating bias corrections either before or during modelling.

Summary: Bias is systematic and matters more than basic poll characteristics: its inclusion triples explanatory power (R^2^ jumps from 14% to 41%). This strongly justifies the decision to compute and apply correction strategies. Still, party-level factors contribute meaningfully, indicating both institutional and methodological influences on poll estimates. This model sets a foundational justification for everything that follows: if bias is systematic, it can be modeled, and corrected.

=\> we add an “OTHER” party bucket for truly new parties =\> if 2023 has unseen parties, we must give the model a catch-all level it learned during training

```{r}
library(forcats)

df_hist_2 <- df_hist
df_2023_2 <- df_2023

# 1) Ensure training knows an "OTHER" level (even if unused)
df_hist_2$abbrev_candidacies <- fct_expand(factor(df_hist_2$abbrev_candidacies), "OTHER")

# 2) Collapse any test-only parties into OTHER
df_2023_2$abbrev_candidacies <- fct_other(
  factor(df_2023_2$abbrev_candidacies),
  keep = setdiff(levels(df_hist_2$abbrev_candidacies), "OTHER"),
  other_level = "OTHER"
)

# 3) Align levels exactly
df_2023_2$abbrev_candidacies <- factor(df_2023_2$abbrev_candidacies,
                                     levels = levels(df_hist_2$abbrev_candidacies))

```

### 4. Interaction model with debiased estimates: check for residual bias after correction

```{r, eval=F}
desc_m4_residual_bias <- lm(debiased_estimate_final ~
                              polling_firm * abbrev_candidacies,
                            data = data_2023_desc)

summary(desc_m4_residual_bias)
```

Aim: test whether the correction method neutralised bias or whether residual polling behaviour remains (= if the estimates themselves are still biased post-correction). If polling_firm is not significant, suggests neutralisation of pollster differences, vs if polling_firm is still significant, suggests residual bias and that the correction does not capture everything.

> This diagnostic model tests whether the debiasing worked: If polling_firm and its interaction with abbrev_candidacies (party) are not significant, this suggests the correction neutralised house effects. If polling firms still differ significantly, it means residual bias remains, and the correction was incomplete.

Key findings: TODO: look at it individually

-   Model fit:

    -   R^2^: 0.978 = the model explains \~98% of the variance, but that’s expected since debiased estimates are mechanically constructed from actual results + avg error =\> they are supposed to be close to the real thing.

Summary: Main conclusion: After debiasing, polling firm no longer adds explanatory power. This means that **house effects appear successfully neutralised by the correction**. The absence of significant pollster or interaction effects confirms the diagnostic goal of this model: debiasing made pollsters indistinguishable in their estimates. Therefore, the model supports the validity of the correction method used (based on average error), and shows that **further pollster-based adjustments may not be needed in predictive modelling**, unless non-linearities or interactions emerge elsewhere.

-   The presence of many dropped coefficients (due to singularities) suggests that some party-pollster combinations were too sparse to estimate independently =\> so caution is still warranted in small-sample pollsters.

# slow rf model

```{r, eval=F}

# SLOW MODEL ---------------------------------------



# Zero Variance check
nzv_check <- nearZeroVar(df_hist, saveMetrics = TRUE)
print(nzv_check)
sum(nzv_check$zeroVar) # 0
sum(nzv_check$nzv) # 0 

# if (any(nzv_check$nzv)) {
  # training_noVar <- training[, !nzv_check$nzv]
  # testing_noVar  <- testing[, colnames(training_noVar)]  # align columns
# }


# tree grid 
ntree_values <- c(100, 250, 500, 750,)

rf_results_during <- tibble(ntree = integer(),  
                            RMSE = numeric(), 
                            Rsquared = numeric()
                            )

# keeping models to retrieve the best one (and not have to wait again)
list_m_rf_during <- vector("list", length(ntree_values))
names(list_m_rf_during) <- paste0("nt", ntree_values)

# loop 
for (i in seq_along(ntree_values)) {
   
  nt <- ntree_values[i]
  set.seed(70901) 
  
  m_rf_during <- train(voting_results_pct ~ 
                         estimated_voting + correction_final + 
                         n_field_days + 
                         sample_size + 
                         party_elec_count + 
                         first_time, 
                       data = df_hist,
                       method = "rf",
                       trControl = ctrl_loeo,
                       preProcess = c("center","scale"), # theoretically not requiered 
                       ntree = nt,
                       tuneGrid = data.frame(mtry = c(1, 2, 3, 4, 5)),
                       importance = TRUE
                    )
  
  # best row from carets resampled results
  best_row <- m_rf_during$results %>%
    arrange(RMSE) %>%
    slice(1)
  
  rf_results_during <- bind_rows(
    rf_results_during,
    tibble(
      ntree = nt,
      RMSE = best_row$RMSE,
      Rsquared = best_row$Rsquared
    )
  )
  
  # storing model
  list_m_rf_during[[paste0("nt", nt)]] <- m_rf_during
  
}


rf_results_during_test1 <- rf_results_during


saveRDS(rf_results_during_test1, file = "rf_results_during_test1.rds")
```

```{r}
rf_results_during_keep <- data.frame(
  ntree = c(100, 250, 500, 750),
  RMSE = c(4.939683, 4.906930, 4.934626, 4.936912),
  Rsquared = c(0.9016610, 0.9032702, 0.9027997, 0.9032170)
)

rf_results_during_keep
```

# wrong models

#### intercepts and slopes plotes

NOTE: THESE ARE THE SAME PLOT INFOS THAN FOR THE PLOTS DIRECTLY DONE AFTER MM3

```         
- => They visualise the random effects (intercepts and slopes) from the same mixed-effects model (model_mixed_slopes / model_mixed_3_varying_slopes).
```

visualise the firm-party interaction effects from mixed-effects model =\> extract and plot the random intercepts and slopes.

How pollsters differ in their baseline bias per party (intercepts) How pollsters differ in how they track real vote shares for each party (slopes)

=\> Extract Random Effects (Intercepts and Slopes)

```{r}
library(lme4)
library(broom.mixed)
library(dplyr)

# Extract random effects with estimates and confidence intervals
ranef_data <- ranef(model_mixed_slopes, condVar = TRUE)$`polling_firm:abbrev_candidacies` %>%
  as.data.frame() %>%
  tibble::rownames_to_column("firm_party") %>%
  rename(intercept = `(Intercept)`, slope = voting_results_pct)

# Add pollster and party as separate variables
ranef_data <- ranef_data |>
  separate(firm_party, into = c("polling_firm", "abbrev_candidacies"), sep = ":", remove = FALSE)
```

=\> Random Intercepts (Baseline Bias)

```{r}
library(ggplot2)

ranef_data |>
  ggplot(aes(x = reorder(polling_firm, intercept), y = intercept, fill = abbrev_candidacies)) +
  geom_col(position = position_dodge(width = 0.9)) +
  coord_flip() +
  labs(title = "Random Intercepts: Baseline Bias by Firm and Party",
       x = "Polling Firm",
       y = "Estimated Intercept (Bias)",
       fill = "Party") +
  theme_minimal()


```

=\> Random Slopes (Responsiveness to Real Vote Share)

```{r}
ranef_data |>
  ggplot(aes(x = reorder(polling_firm, slope), y = slope, fill = abbrev_candidacies)) +
  geom_col(position = position_dodge(width = 0.9)) +
  coord_flip() +
  labs(title = "Random Slopes: Vote Share Responsiveness by Firm and Party",
       x = "Polling Firm",
       y = "Estimated Slope",
       fill = "Party") +
  theme_minimal()

```

=\> Facetted View: Bias per Party across Pollsters

```{r}
ranef_data |>
  ggplot(aes(x = intercept, y = reorder(polling_firm, intercept))) +
  geom_col() +
  facet_wrap(~ abbrev_candidacies, scales = "free_y") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(title = "Baseline Pollster Bias by Party (Random Intercepts)",
       x = "Intercept (Bias)", y = "Polling Firm") +
  theme_minimal()

```

=\> adding Confidence Intervals to Random Effects Plots

```{r}
# Extract random effects with conditional variances
re <- ranef(model_mixed_3_varying_slopes, condVar = TRUE)

# Get standard errors
se_list <- attr(re$`polling_firm:abbrev_candidacies`, "postVar")
n <- dim(se_list)[3]

re_df <- as.data.frame(re$`polling_firm:abbrev_candidacies`) %>%
  rownames_to_column("firm_party") %>%
  rename(
    intercept = `(Intercept)`,
    slope = voting_results_pct
  ) %>%
  separate(firm_party, into = c("polling_firm", "abbrev_candidacies"), sep = ":", remove = FALSE) %>%
  mutate(
    se_intercept = sqrt(sapply(1:n, function(i) se_list[1, 1, i])),
    se_slope = sqrt(sapply(1:n, function(i) se_list[2, 2, i])),
    ci_lower_intercept = intercept - 1.96 * se_intercept,
    ci_upper_intercept = intercept + 1.96 * se_intercept,
    ci_lower_slope = slope - 1.96 * se_slope,
    ci_upper_slope = slope + 1.96 * se_slope
  )

```

Plols with confidence intervals

```{r, eval=F}
# rank by effect magnitue
re_df <- re_df %>%
  group_by(abbrev_candidacies) %>%
  mutate(polling_firm = fct_reorder(polling_firm, abs(slope))) %>%
  ungroup()
```

```{r}
# intercept 
ggplot(re_df, aes(x = intercept, y = fct_reorder(polling_firm, intercept))) +
  geom_point() +
  geom_errorbarh(aes(xmin = ci_lower_intercept, xmax = ci_upper_intercept ), height = 0.2) +
  geom_vline(xintercept = fixef(model_mixed_3_varying_slopes)["voting_results_pct"], linetype = "dashed", colour = "grey50") +
  facet_wrap(~ abbrev_candidacies, scales = "free_y") +
  labs(
    title = "Intercepts by Polling Firm (with 95% CI, facetted by Party)",
    x = "Intercepts on Voting Results %",
    y = "Polling Firm"
  ) +
  theme_minimal(base_size = 12)

# slopes
ggplot(re_df, aes(x = slope, y = fct_reorder(polling_firm, slope))) +
  geom_point() +
  geom_errorbarh(aes(xmin = ci_lower_slope, xmax = ci_upper_slope), height = 0.2) +
  geom_vline(xintercept = fixef(model_mixed_3_varying_slopes)["voting_results_pct"], linetype = "dashed", colour = "grey50") +
  facet_wrap(~ abbrev_candidacies, scales = "free_y") +
  labs(
    title = "Random Slopes by Polling Firm (with 95% CI, facetted by Party)",
    x = "Slope on Voting Results %",
    y = "Polling Firm"
  ) +
  theme_minimal(base_size = 12)

```

## lm

### lm1

linear model 1: estimating bias (error = estimated_voting - voting_results_pct)

-   modelling the polling error directly, by polling firm, party and party-level predictors
-   again: positive coefficients = sistematic overestimation, negative = underestimation

```{r}
model1 <- lm(I(estimated_voting - voting_results_pct) ~ 
               + polling_firm + n_field_days + abbrev_candidacies + party_age + party_elec_count + first_time,
             data = survey_elections)

summary(model1)
```

! remember: all polling_firm coefficients are now relative to our dummy polltser =\> how much more or less biased each firm is compared to the average bias of all pollsters combined

results:

-   R^2^ = 0.203 =\> 20.3% of the variation in polling error is explained
-   house effects (relative to dummyPollster!): firms with statistically significant and strong deviation from the average error:
    -   Overestimating (positive bias)
        -   El País: +16.4pp \*\*\*, Emopublica: +9.5pp \*\*\*, Sofemasa: +10.3pp \*\*\*, ICSA-Gallup: +9.0pp \*\*\*, Alef: +9.0pp \*\*\*, ECO: +5.3pp \*\*\*, Asturbarometro: +4.4pp \*\*\*, Emopublica-Burke: +3.8pp \*\*, CS (party poll): +4.2pp \*\*\*
    -   Underestimating (negative bias)
        -   ASEP: –2.6pp \*\*\*, Opina: –1.0pp \*\*\*, Obradoiro de Socioloxia: –0.85pp \*\*\*, Sigma Dos – Vox Publica: –2.4pp \*\*, Tabula V: –2.6pp \*\*\*, SW Demoscopia: –1.3pp \*\*\*, Vox Publica: –1.4pp \*\*, OTR-IS: –3.3pp \*\*\*
-   Party level effects: significant overestimation or underestimation of specific parties (vs average):
    -   PODEMOS: +5.2pp, \*\*\* =\> systematically overestimate
    -   PSOE: +1.6pp, \*
    -   ERC: +1.9pp, \*\*
    -   PRC: –1.7pp, \*
    -   FN: –4.2pp, \*\*\*
    -   PCE: –1.8pp, \*
    -   BNG: +1.9pp, \*\*
    -   PNV: +1.8pp, \*\*
-   party characteristics:
    -   `party_age`: +0.08pp/year, p \< 0.001 =\> older parties are slightly overestimated
    -   `party_elec_count`: –0.57pp per prior election, p \< 0.001 =\> frequent contenders are less overestimated
        -   not necessarily clashes with party_age!! older does not mean more frequent necessarily... but maybe it does?
    -   `first_time`: –0.77pp, p \< 0.001 =\> new parties are underestimated

PLOT:

```{r}

pollster_effects <- tidy(model1) %>%
  filter(grepl("^polling_firm", term)) %>%
  mutate(
    polling_firm = gsub("^polling_firm", "", term),
    polling_firm = trimws(polling_firm)
  )

top_pollsters <- pollster_effects %>%
  arrange(desc(estimate)) %>%
  slice_head(n = 5) %>%
  bind_rows(
    pollster_effects %>%
      arrange(estimate) %>%
      slice_head(n = 5)
  )

ggplot(top_pollsters, aes(x = reorder(polling_firm, estimate), y = estimate)) +
  geom_col(fill = "steelblue") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  coord_flip() +
  labs(
    title = "Top Pollsters' House Effects vs DummyPollster (Avg as Baseline)",
    x = "Polling Firm",
    y = "Average Error Relative to DummyPollster (percentage points)"
  ) +
  theme_minimal()


```

### lm2

linear model 2: estimating poll predictions, controlling for the actual results and party features =\> we use the actual results right now because we are interesting in bias estimation, not forecasting for now. but for the predictive models we obviously cant use the actual results as predictors!

-   modelling how the polls are formed
-   coefficient for voting_results_pct should be close to 1 if polls track reality well
-   polling_firm coefficients show house effects, controlling for actual results and party features
-   party_age, first_time, etc. =\> explain systematic differences in polling performance by party characteristics.

```{r}
model2 <- lm(estimated_voting ~ 
               voting_results_pct + polling_firm + n_field_days + abbrev_candidacies + party_age + party_elec_count + first_time,
            data = survey_elections)

summary(model2)
```

results:

-   R^2^ = 0.936 =\> the model explains **93.6%** of the variation in vote estimates =\> wtf
-   voting_results_pct: β = 0.679, p \< 0.001 =\> polls track actual results, but tend to dampen differences (slope \< 1), pulling estimates toward the mean
-   significant polling firm effects (vs dummyPollster):
    -   Overestimating
        -   El País: +13.4pp \*\*\*, Emopublica: +7.6pp \*\*\*, Sofemasa: +8.4pp \*\*\*, ICSA-Gallup: +6.6pp \*\*\*, ECO: +4.1pp \*\*\* Emopublica-Burke, Ideal, OT-Press: all ≥ 5pp, p \< 0.001
    -   Underestimating
        -   ElectoPanel: –0.68pp \*\*\*, Opina: –0.28pp \*\*, OTR-IS: –2.5pp \*\*\*, Sigma Dos – Vox Pública: –1.6pp \*
-   party level effects: consistently over- or under-estimated in polls, even after controlling for results
    -   PODEMOS: +7.4pp \*\*\*
    -   PSOE: +11.3pp \*\*\*
    -   PP: +11.1pp \*\*\*
    -   VOX: +5.1pp \*\*\*
    -   PRC: –7.4pp \*\*\*
-   party characteristics:
    -   `party_age`: +0.35pp/year, p \< 0.001\
        =\> Older parties are systematically more overestimated
        -   **like first model**
    -   `party_elec_count`: –1.58pp per election, p \< 0.001\
        =\> Well-established parties are harder to overinflate / frequent contenders are less overestimated
        -   **like first model**
    -   `first_time`: –1.17pp, p \< 0.001\
        =\> New parties are underestimated
        -   **like first model**

regression based house effects table:

```{r}
house_effects2 <- tidy(model2) |>
  filter(grepl("^polling_firm", term)) # estimated bias (in percentage points) per polling firm relative to the reference firm



house_effects2
```

### lm3 polling_firm\*abbrev_candidacies

-   voting_results_pct: controls for actual vote share

-   interaction

    -   polling_firm main effects (baseline bias by firm)
    -   abbrev_candidacies main effects (systematic over/under by party)
    -   Interaction terms: specific adjustments for each firm-party combination =\> polling_firm \* abbrev_candidacies: estimates pollster-party-specific biases

-   n_field_days, party_age, party_elec_count, first_time: account for fieldwork conditions and party characteristics.

idea: separating methodological and partisan effects from real polling bias

=\> Do certain firms favour certain parties, even after controlling for objective factors?

```{r}
# JAVI 1


# abbrev candidacies => estimate a max number => filter by vote count => min. 3% of vote share 
# other option: manual selection 



# filtering dataset 
party_counts <- survey_elections %>% 
  count(abbrev_candidacies, sort = TRUE) %>% 
  slice_max(n, n = 10)
party_counts

pollster_counts <- survey_elections %>% 
  count(polling_firm, sort = TRUE) %>% 
  slice_max(n, n = 11) # 11 por el dummy pollster 
pollster_counts

filtered_data <- survey_elections %>% 
  filter(
    abbrev_candidacies %in% party_counts$abbrev_candidacies,
    polling_firm %in% pollster_counts$polling_firm
  )

# interaction model with firm × party interaction
model3_interaction <- lm(
  estimated_voting ~ voting_results_pct + polling_firm * abbrev_candidacies + n_field_days + party_age + party_elec_count + first_time,
  data = filtered_data
)




summary(model3_interaction)
```

results:

-   High R^2^ (0.965): model explains nearly all the variation in estimated vote share =\> great predictive fit =\> ????? weird

-   Interaction terms: Many polling firm × party combinations are statistically significant, confirming that house effects are not uniform across parties. E.g.:

    -   ElectoPanel:PSOE and ElectoPanel:PP show large negative coefficients, meaning strong underestimation.
    -   Simple Logica:PODEMOS and Simple Logica:CS show large positive coefficients, suggesting consistent overestimation.

-   Some NA interactions: Due to no data for specific pollster-party pairs

```{r}
# test with full data 
# interaction model with firm × party interaction
model3_interaction_full <- lm(
  estimated_voting ~ voting_results_pct + polling_firm * abbrev_candidacies + n_field_days + party_age + party_elec_count + first_time,
  data = survey_elections
)


summary(model3_interaction_full)
```

#### plots

plot that clearly shows the polling_firm × party interaction effects — i.e., how much each firm over/underestimates each party relative to the baseline

```{r}
library(stringr)

# extracting interaction terms
interaction_effects <- tidy(model3_interaction) %>% 
  filter(str_detect(term, ":")) %>% 
  mutate(
    firm = str_extract(term, "(?<=polling_firm)[^:]+"),
    party = str_extract(term, "(?<=abbrev_candidacies)[^:]+")
    )

# Plot: firm × party effects
ggplot(interaction_effects, aes(x = estimate, y = interaction(party, firm), colour = estimate > 0)) +
  geom_point() +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(
    title = "Interaction Effects: Polling Firm × Party",
    x = "Estimated Over/Underestimation (pp)",
    y = "Party • Pollster",
    colour = "Overestimate?"
  ) +
  theme_minimal(base_size = 12)
```

how each pollster deviates in their estimates for each party compared to the baseline

Dots to the right of zero = overestimation, to the left = underestimation

=\> by pollster

```{r}
# reorder firms by average estimate
interaction_effects <- interaction_effects |>
  group_by(firm) |>
  mutate(mean_estimate = mean(estimate, na.rm = TRUE)) |>
  ungroup() |>
  mutate(firm = fct_reorder(firm, mean_estimate))

# Plot
ggplot(interaction_effects, aes(x = estimate, y = fct_reorder(party, estimate), fill = estimate > 0)) +
  geom_col(show.legend = FALSE) +
  geom_vline(xintercept = 0, linetype = "dashed", colour = "grey40") +
  facet_wrap(~ firm, scales = "free_y") +
  labs(
    title = "Interaction Effects by Polling Firm and Party",
    x = "Estimation Error (pp)",
    y = "Party"
  ) +
  scale_fill_manual(values = c("TRUE" = "steelblue", "FALSE" = "tomato")) +
  theme_minimal(base_size = 12)

```

Adjusts for covariates like actual results, party characteristics, fieldwork Coefficients reflect bias after controlling for other factors

=\> by parties (similar to the top10top10 plot of before, but having controlled for the covariates)

```{r}

ggplot(interaction_effects, aes(x = estimate, y = fct_reorder(firm, estimate), fill = estimate > 0)) +
  geom_col(show.legend = FALSE) +
  geom_vline(xintercept = 0, linetype = "dashed", colour = "grey40") +
  facet_wrap(~ party, scales = "free_y") +
  labs(
    title = "Pollster Bias by Party (Interaction Model)",
    x = "Estimation Error (pp)",
    y = "Polling Firm"
  ) +
  scale_fill_manual(values = c("TRUE" = "steelblue", "FALSE" = "tomato")) +
  theme_minimal(base_size = 12)

```

interpretation:

-   we can compare it to the top10top10 bias estimate, that used only the raw house effects!
-   TODO lol

## mixed / random effects model

HERE

only polling firm in the random effects

introduce non-linear patterns - random forest, neural networks?

separar por elecciones (encuestas solo dentro de esa eleccion) y loop y resultados

ranking de encuestadores (MAE) por average house effect pollster x party + days until election =\> weight

-   incluir variable como predictora
-   o
-   usar antes =\> media ponderada antes =\> eso entra en el modelo

= promedio de referencia

=\> luego hacer los house effects en base del promedio?

### mm1 polling_firm = random intercept

why?

-   we believe pollsters have individual biases (house effects) but prefer to model them as a distribution, not fixed constants =\> leads to more stable, generalisable estimates, especially when some pollsters have little data

-   "Each polling firm has its own average bias, but I’m not interested in estimating each one individually. Instead, I assume these firm effects are randomly drawn from a common distribution"

-   coefficients: empirical Bayes estimates: the most likely random intercept for each firm given the data and the estimated variance of the group =\> not coefficients but shrunk estimates

model:

-   fixed effects = variables until now
-   random intercept `(1 | polling_firm)` = allows each polling firm to have its own bias around the overall mean
    -   "allows **polling firms to deviate randomly** from the fixed-effect prediction — this models house effects as random deviations rather than fixed"

```{r}
model_mixed_1 <- lmer(
  estimated_voting ~ voting_results_pct + n_field_days + 
    party_age + party_elec_count + first_time + 
    (1 | polling_firm),
  data = survey_elections
)

summary(model_mixed_1)


```

reuslts:

-   REML = 269294 (lower is better)
-   Polling firm variance = 6.87 =\> substantial variation in baseline predictions across firms
-   random effects / house effect: random intercepts allow each polling firm to deviate from the overall average
    -   `polling_firm` SD = 2.62pp\
        =\> Polling firms vary around the global mean by \~2.6pp — this captures house effects as random deviations
-   fixed effects:
    -   Intercept (3.74): On average, polls slightly overestimate vote shares when all predictors are at zero.
    -   Voting results (%): 0.94 =\> Strong linear relationship with poll estimates; nearly 1-to-1 tracking of actual results.
    -   n_field_days: 0.0011 =\> No meaningful or significant effect from how many days the fieldwork lasted.
    -   party_age: –0.14 =\> Older parties are systematically underestimated in polls.
        -   **different than models before**
    -   party_elec_count: +0.33 =\> Parties that have participated in more elections tend to be estimated higher.
        -   **different than models before**
    -   first_time: –0.68 =\> New parties are significantly underestimated by almost 0.7pp.
        -   **same as models before**

General:

-   **Polls closely follow results (0.94 slope)**, but **systematic biases remain**: Newer parties are underestimated, Polling houses vary significantly in their baseline bias (Polling firms differ meaningfully in their biases — house effects are real and measurable)

PLOT:

```{r}

pollster_effects <- ranef(model_mixed_1, condVar = TRUE)$polling_firm %>%
  tibble::rownames_to_column("polling_firm") %>%
  rename(house_effect = `(Intercept)`)

se <- attr(ranef(model_mixed_1, condVar = TRUE)$polling_firm, "postVar")[1, 1, ]
pollster_effects <- pollster_effects %>%
  mutate(
    se = sqrt(se),
    ci_lower = house_effect - 1.96 * se,
    ci_upper = house_effect + 1.96 * se
  )


top_pollsters <- pollster_effects %>%
  arrange(desc(abs(house_effect))) %>%
  slice_head(n = 20)  # top 20 most biased firms (positive or negative)

ggplot(top_pollsters, aes(x = reorder(polling_firm, house_effect), y = house_effect)) +
  geom_col(fill = "steelblue") +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.3) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  coord_flip() +
  labs(
    title = "Estimated Polling Firm House Effects (Random Intercepts)",
    x = "Polling Firm",
    y = "Deviation from Average Estimate (pp)"
  ) +
  theme_minimal()

```

### mm2 pollster and parties = random slopes

-   `polling_firm` random intercept =\> models house effects as deviations from the average

-   `abbrev_candidacies` random intercept =\> models party-specific deviations (e.g., some parties are systematically harder to poll accurately)

-   the fixed effects still control for actual vote share and party characteristics

```{r}
model_mixed_2 <- lmer(
  estimated_voting ~ voting_results_pct + n_field_days + 
    party_age + party_elec_count + first_time + 
    (1 | polling_firm) + (1 | abbrev_candidacies),
  data = survey_elections
)

summary(model_mixed_2)
```

reuslts:

-   REML = 256,757 (lower than previous model = better fit)
-   Residual SD = 3.49pp =\> similar overall predictive accuracy
-   account for biases by pollster AND by party, improving generalisation and reducing overfitting
    -   **miau but we do not actually want generalisation do we? we want specific data for each pollster and each party, but still this is all quite interesting... maybe**
-   random effects:
    -   **Polling firm variance**: 4.52 (SD ≈ 2.13pp)\
        =\> Confirms meaningful house effects across firms
    -   **Party (abbrev_candidacies) variance**: 11.34 (SD ≈ 3.37pp)\
        =\> Shows substantial variation in baseline vote estimates across parties, even after accounting for actual vote share and party characteristics
        -   **some parties are consistently over-/underestimated, regardless of who runs the poll =\> parties are harder to poll!**
-   Fixed effects:
    -   Intercept: +3.74pp =\> baseline overestimation remains
    -   Voting results (%): +0.68 =\> polls still underreact to real variation (slope \< 1)
    -   n_field_days: –0.006 =\> longer fieldwork slightly reduces estimates (small but significant)
        -   **first time n_field_days is significant!**
    -   party_age: +0.34pp/year =\> older parties are more overestimated
    -   party_elec_count: –1.56pp/election =\> parties with long histories tend to be underestimated
        -   **different than before**
    -   first_time: –1.15pp =\> new parties remain significantly underestimated
        -   **as before**

PLOT: random effects for parties (abbrev_candidacies, party-level bias)

```{r}
party_effects <- ranef(model_mixed_2)$abbrev_candidacies %>%
  as.data.frame() %>%
  rename(party_bias = `(Intercept)`) %>%
  mutate(abbrev_candidacies = rownames(ranef(model_mixed_2)$abbrev_candidacies)) %>%
  arrange(desc(party_bias))

ggplot(party_effects, aes(x = reorder(abbrev_candidacies, party_bias), y = party_bias)) +
  geom_col(fill = "darkred") +
  coord_flip() +
  labs(
    title = "Party-Level Random Effects (Bias)",
    x = "Party",
    y = "Estimated Bias (Intercept)"
  ) +
  theme_minimal()

```

```{r}
# firm vs party bias magnitudes (sd / random intercepts)
re_variance <- as.data.frame(VarCorr(model_mixed_2))
re_variance %>% 
  filter(grp %in% c("polling_firm", "abbrev_candidacies")) %>%
  select(grp, sdcor) %>%
  rename(Random_Effect = grp, Std_Deviation = sdcor)

```

### mm3

mixed-effects model with varying slopes per party by polling firm, which allows house effects to vary by party within each firm in a more flexible, hierarchical way

=\> GOAL: capture how much each polling firm deviates from the average, per party, but using partial pooling (shrinkage), improving estimates for firms with few observations

-   polling_firm:abbrev_candidacies defines a firm–party pair as the group

-   (1 + voting_results_pct \| ...) lets both the intercept (bias) and slope (responsiveness) vary for each firm-party combo.

-   Allows for:

    -   Some firms consistently overestimating certain parties (intercept).
    -   Some firms being more or less sensitive to actual vote share (slope).

==\> Fixed effects: overall relationships (e.g. average slope for voting_results_pct)

==\> Random effects (by firm–party pair): how much each firm-party deviates from the average

```{r}

model_mixed_3_varying_slopes <- lmer(
  estimated_voting ~ voting_results_pct + n_field_days + 
    party_age + party_elec_count + first_time +
    (1 + voting_results_pct | polling_firm:abbrev_candidacies),
  data = filtered_data # filtered data comes from lm3, we are using only the top 10 pollster and top 10 parties 
)

summary(model_mixed_3_varying_slopes)

```

reuslts:

-   Fixed Effects (overall trends across all firms and parties): Intercept = 7.43pp: average baseline vote estimate when all predictors are zero.

    -   **voting_results_pct = 0.61: for each 1pp increase in actual vote share, the predicted poll estimate increases by 0.61pp on average → polls under-react to actual support**
    -   party_age = +0.22: older parties tend to receive slightly higher estimated support
    -   party_elec_count = -0.86: more electoral history =\> slightly lower estimates (possibly due to smaller/declining parties)
    -   first_time = -0.60: first-time national parties are estimated lower in polls.

-   Random Effects (firm–party variation): Intercept SD = 8.83pp: strong variation in baseline estimates across firm-party pairs (i.e. some consistently over- or under-estimate).

    -   Slope SD for voting_results_pct = 0.45: firms differ slightly in how much they scale poll estimates with actual vote share.
    -   Correlation = -0.78: strong negative correlation between intercept and slope — e.g. firms that consistently overestimate may also under-react to changes in vote share

-   Model fit:

    -   Residual SD = 2.15pp: after accounting for all fixed + random effects, poll estimates deviate \~2.15pp from observed values.
    -   Groups = 99 firm–party pairs used (i.e. interactions with data available).

#### plot

extract random effects

```{r}
# Extract random effects with confidence intervals
re <- ranef(model_mixed_3_varying_slopes, condVar = TRUE)


# Convert to dataframe
re_df <- as.data.frame(re$`polling_firm:abbrev_candidacies`) %>% 
  rownames_to_column("firm_party") %>% 
  rename(
    intercept = `(Intercept)`,
    slope = voting_results_pct
  ) %>% 
  separate(firm_party, into = c("polling_firm", "abbrev_candidacies"), sep = ":", remove = FALSE)
```

plot: random intercepts ( baseline bias per firm-party) by party facetted

```{r}
ggplot(re_df, aes(x = intercept, y = fct_reorder(polling_firm, intercept))) +
  geom_point() +
  geom_vline(xintercept = 0, linetype = "dashed", colour = "grey50") +
  facet_wrap(~ abbrev_candidacies, scales = "free_y") +
  labs(
    title = "Random Intercepts by Polling Firm (facetted by Party)",
    x = "Baseline Bias (Intercept)",
    y = "Polling Firm"
  ) +
  theme_minimal(base_size = 12)

```

plot: random slopes (sensitivity to actual vote share) by party facetted

```{r}
ggplot(re_df, aes(x = slope, y = fct_reorder(polling_firm, slope))) +
  geom_point() +
  geom_vline(xintercept = fixef(model_mixed_3_varying_slopes)["voting_results_pct"],
             linetype = "dashed", colour = "grey50") +
  facet_wrap(~ abbrev_candidacies, scales = "free_y") +
  labs(
    title = "Random Slopes by Polling Firm (facetted by Party)",
    x = "Slope on Voting Results %",
    y = "Polling Firm"
  ) +
  theme_minimal(base_size = 12)


```

# advanced models

target var: polling_error = estimated_voting - voting_results_pct

```{r}
data <- survey_elections
```

## regression ready dataset

filter out missing values =\> need for clean training and testing process

```{r data_regg}
data_regg <- data %>% 
  filter(!is.na(estimated_voting), !is.na(voting_results_pct)) %>% 
  mutate(
    polling_error = estimated_voting - voting_results_pct,
    first_time = factor(first_time),
    
    # reduce noise, avoid overfitting (??) => grouping into "OThers" level 
    abbrev_candidacies = fct_lump_min(abbrev_candidacies, 5),
    polling_firm = fct_lump_min(polling_firm, 5)
    
  ) %>% 
  select(
    polling_error,
    estimated_voting,
    voting_results_pct,
    polling_firm,
    abbrev_candidacies,
    n_field_days,
    party_age,
    party_elec_count,
    first_time
  ) %>% 
  drop_na()


cat("Regression data:", nrow(data_regg), "observations\n")
```

23994 obs

## Multiple regression

split training 80% and testinf 20%

```{r MR dataPartition}
set.seed(123)
in_train <- createDataPartition(data_regg$polling_error, p = 0.80, list = FALSE) 
training <- data_regg[in_train,]
testing <- data_regg[-in_train,]
```

correlations:

```{r MR correlated vars}
x <- training %>% 
  select(estimated_voting, voting_results_pct, n_field_days, party_age, party_elec_count) %>% 
  cor(use = "complete.obs") %>% 
  round(2) 

x

# library(corrplot)
corrplot::corrplot(cor(x))

```

scaling

```{r}
train_scaled <- train_data |>
  mutate(across(c(voting_results_pct, n_field_days, party_age, party_elec_count), scale))

test_scaled <- test_data |>
  mutate(across(c(voting_results_pct, n_field_days, party_age, party_elec_count), scale))

```

lm another try

```{r}
lm_basic <- lm(estimated_voting ~ voting_results_pct + n_field_days +
                 party_age + party_elec_count + first_time,
               data = train_scaled)

summary(lm_basic)
```

....

automated model selection (stepwise) statistical tools (lasso, etc) ml methods

# accounting for house effects

The goal is to create models that account for the party variation by pollster.
